<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-13T23:01:25+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">끊임없이 성장하는 데이터 분석가</title><subtitle>조금씩 쌓아가다 보면 큰 산이 되겠지</subtitle><author><name>박마토</name></author><entry><title type="html">[회고록] 23년 3월 2주차 회고록</title><link href="http://localhost:4000/data/weekly_memoir/" rel="alternate" type="text/html" title="[회고록] 23년 3월 2주차 회고록" /><published>2023-03-13T00:00:00+09:00</published><updated>2023-03-14T06:21:00+09:00</updated><id>http://localhost:4000/data/weekly_memoir</id><content type="html" xml:base="http://localhost:4000/data/weekly_memoir/"><![CDATA[<p>  첫 주차별 회고록입니다. 현재 프로젝트 초반기라 정신없이 지내고 있는데, 기록하고 싶은 것들이 있어 이렇게 회고록을 작성합니다. 이번 회고록은 두 가지 키워드를 중심으로 적고자 하는데, 하나는 <strong>데이터의 품질</strong>과 다른 하나는 <strong>에너지 분석</strong>입니다.</p>

<p>  본격적인 회고록을 작성하기 전에 현재 어떤 프로젝트를 진행중인지 간단하게 설명하겠습니다. 저는 현재 제조 공장에서 데이터 분석을 통해 에너지 사용량 절감 포인트를 도출하고 개선하는 프로젝트를 진행중입니다. 제조 공장에서는 다양한 에너지를 사용하며 제품을 생산하는데, 대표적으로 스팀, 전력, 가스 등이 있습니다. 에너지 사용량 데이터와 관련된 인자들의 데이터를 수집하여 데이터 모델을 만들고 열심히 EDA를 하면서 에너지 절감 포인트를 도출하고 있습니다.</p>

<h2 id="1-데이터-품질">1. 데이터 품질</h2>
<p>  현재 프로젝트를 진행하고 있는 제조 공장은 가동된지 오래된 연식 있는 공장입니다. 따라서, 아직 많은 데이터를 수기로 입력하여 관리하고 있는 실정이더군요. 이 프로젝트에서 가장 중요한 에너지 사용량 역시 엑셀 파일에 수기로 입력하여 관리하고 있었습니다. 이러한 상황이다보니 특정 데이터의 품질이 매우 떨어져 신뢰하기 어려운 상황이 발생했다. 엑셀로 제공 받은 데이터의 값을 검증하다가 프로젝트 매니저인 책임님꼐서 이상한 부분을 발견하셨다. 다른 소스에서 데이터를 가져와 연결해보니 매칭이 안되는 것들이 너무 많은 것이었다. 이슈가 발생했다. 나는 진행하던 분석을 멈추고 해당 데이터를 뜯어보기 시작했습니다. <br />
<br />
  데이터 원천인 엑셀 데이터를 다시 하나하나 뜯어보고, 다른 소스에서 데이터를 가져와 맞춰보고, 공장 엔지니어에게 문의하고 정리하여 내린 결론은 문제가 생긴 특정 데이터를 도저히 분석에 활용하기 어렵다는 것입니다. 이 데이터는 에너지 관점에서는 매우 중요한 데이터였기에 너무나 아쉬운 상황이 됐습니다. 이 데이터는 일별 생산량 데이터인데, 생각을 해보자. 생산을 많이 한다는 것은 공장 가동을 많이 한다는 것이고 그 만큼 에너지를 많이 사용하게 됩니다. 물론, 에너지 사용량이 생산량과 연관이 있는지는 데이터로 확인해봐야 합니다다. 꼭 그렇지 않은 경우도 많기 때문이죠. 에너지 사용 분석을 위해서 보통 사용량과 원단위(사용량/생산량) 지표를 많이 활용하는데 우린 원단위 분석이 불가능해졌습니다. 안되는 건 어쩔 수 없는거고 사용량만을 가지고 분석을 진행하기로 했습니다. <br /><br />
  데이터 품질에 관한 문제는 진행했던 거의 모든 프로젝트에서 발생하더군요. 주로 1. 분석을 위해 필요한 데이터가 없거나 2. 데이터가 정확하지 않거나 3. 특정한 이유로 데이터는 있는데 가져올 수 없는 경우입니다.</p>

<h3 id="11-데이터가-부재한-경우">1.1 데이터가 부재한 경우</h3>
<p>  실제로 프로젝트를 하다보면 필요한 데이터가 없는 경우가 부지기수이다. 이유는 1. 측정이 어려워서 하지 않거나, 2. 측정 기기의 구입/설치/유지 비용이 너무 많이 들거나 3. 중요한지 몰라서 관리하지 않은 경우가 많습니다. 제가 진행했던 프로젝트를 돌아보면 데이터가 없는 경우, 프로젝트 결론이 다음과 같이 나는 경우가 많았던 것 같습니다.</p>
<blockquote>
  <p><strong>“필요 데이터가 없으니(충분하지 않으니) 데이터를 충분히 수집한 다음에 다시 진행합시다.” 혹은 “필요한 데이터가 없으니 더 이상 진행이 어렵습니다.”</strong></p>
</blockquote>

<p>그렇다고 손 놓고 포기할 순 없죠. 필요한 특정 데이터가 없을 떄, 시도해볼 수 있는 방법은 메커니즘 적으로 대신할 수 있는 다른 데이터를 활용해보는 것입니다. 실제로 여러 인자들을 조합해 인자를 만들어 내는 경우도 많고, 완벽하진 않지만 대안으로 특정 인자를 대변할 수 있는 다른 인자를 활용하기도 합니다. 정 안되면 현재 여건에서 할 수 있는데 까지 해보는 것이겠죠.</p>

<h3 id="12-데이터가-부정확한-경우">1.2 데이터가 부정확한 경우</h3>
<p>  데이터는 있으나 부정확한 경우도 너무너무 많습니다. 이상치(Outlier)인 경우라면 그냥 날리면 되겠지만, 전반적으로 데이터를 신뢰하기 어려울 정도의 상황이라면 그럴수도 없습니다. 근본적인 문제의 원인은 데이터 수집과 관리를 위한 시스템이 아직도 제대로 갖춰져 있지 않기 때문이라고 개인적으로 생각합니다. 옆에서 봤을 때 다음과 같은 이유로 문제가 발생하더군요.</p>
<blockquote>
  <ol>
    <li>체계적인 데이터 수집/관리 시스템 미비</li>
    <li>현장 작업자들이 엑셀 작업에 어려움을 느낌</li>
    <li>관리자들의 업무 로드 과다로 인하여 데이터 관리 수준 저하</li>
    <li>데이터 수집/관리 시스템 도입을 위한 투자에 대해 보수적인 경영진</li>
  </ol>
</blockquote>

<p>  실무자 입장에서는 데이터와 시스템이 어느정도 제대로 갖춰진 상태에서 분석을 진행했으면 좋겠다고 느낄 때가 많습니다. 반면, 경영진 입장에서는 비용을 투자하여 시스템을 도입하기 위해서는 데이터 분석을 통해 성과가 나는 것을 보고 진행하려고 합니다. 경영진 입장도 충분히 이해가 가기에 뭐라하고 싶진 않습니다. 다만, 이번 프로젝트가 잘 되서 더 나아지길 빌 뿐입니다. 단순히 편하자고 하는 말이 아니라, 더 빠르고 더 나은 성과를 얻기 위해서는 가장 먼저 필요한 것이 체계적인 시스템이라 생각합니다. 실제로 여태까지 시간을 분석보다는 데이터 검증에 시간을 쏟았고, 그만큼 진행속도가 더뎌지기 떄문입니다. 예전 프로젝트에서도 오기입 데이터가 너무 많아서 담당자와 앉아서 엑셀 한줄한줄 고쳤던 경험이 있습니다. 만약 이런 에러가 나지 않게끔 시스템을 설계해 도입했다면 낭비를 줄일 수 있었을 거라 생각합니다.</p>

<h2 id="2-에너지-분석">2. 에너지 분석</h2>
<p>  아직 분석 초기 단계라 회고할 내용이 많지 않습니다만, 에너지 관점에서 분석을 진행할 때 중요한 포인트만 몇 가지 적겠습니다. 에너지 사용량을 분석할 때 반드시 고려해야 할 2가지 요소가 있습니다.</p>
<blockquote>
  <ol>
    <li>외기 기온(계절성 여부)</li>
    <li>생산량</li>
  </ol>
</blockquote>

<p>  열과 관련된 에너지는 특히 계절성을 갖는 경우가 많습니다. 예를 들어, 스팀은 추울 때 더 많이 쓰고 더울 때 덜 쓰는 경향이 있습니다. 그리고 에너지 사용량은 생산량과도 연관이 있을 확률이 높습니다. 생산을 많이 할수록 에너지 사용량도 증가할 확률이 높기 때문입니다.<br />
  그래서 에너지 사용량과 외기 기온, 생산량이 상관성이 높을 때, 반드시 외기 기온과 생산량을 고정(통제)시킨 상태에서 에너지 사용량에 산포가 있는 지를 살펴봐야 합니다. 2가지 요소를 통제하고 나서도 에너지 사용에 산포가 큰 경우는 다른 인자로 인하여 낭비 요소가 있다는 뜻 입니다. 현재도 위와 같은 방법으로 에너지 사용량의 산포가 큰 경우를 탐색하고, 어떤 이유로 인해 산포가 큰지 분석을 진행하고 있습니다. 주로 산점도를 그려가면서 분석을 진행하고 있는데 X축의 기온을 Y축의 에너지 사용량을 표시한다면, 동일 기온에서 산포가 얼마나 발생했는지 시각적으로 매우 쉽고 빠르게 확인할 수 있습니다. <br />
  에너지 쪽은 처음 분석해보는 거라 하면서 배우는 것도 많고 재밌게 하고 있습니다. 성공적으로 프로젝트가 진행되길 기원하며, 이번 회고록은 여기까지 하도록 하겠습니다. <br /><br />
<strong>감사합니다.</strong></p>]]></content><author><name>박마토</name></author><category term="Data" /><category term="Data" /><category term="Memoir" /><summary type="html"><![CDATA[  첫 주차별 회고록입니다. 현재 프로젝트 초반기라 정신없이 지내고 있는데, 기록하고 싶은 것들이 있어 이렇게 회고록을 작성합니다. 이번 회고록은 두 가지 키워드를 중심으로 적고자 하는데, 하나는 데이터의 품질과 다른 하나는 에너지 분석입니다.]]></summary></entry><entry><title type="html">[프로젝트회고록] 02. 두번째 회고록: 유의차 분석</title><link href="http://localhost:4000/data/projects_memoir2/" rel="alternate" type="text/html" title="[프로젝트회고록] 02. 두번째 회고록: 유의차 분석" /><published>2023-03-06T00:00:00+09:00</published><updated>2023-03-07T07:43:00+09:00</updated><id>http://localhost:4000/data/projects_memoir2</id><content type="html" xml:base="http://localhost:4000/data/projects_memoir2/"><![CDATA[<p>  안녕하세요. 이번에는 Y인자가 <strong>이항 변수(Binary Variable)</strong>일 때, 어떻게 문제에 접근했고 풀었는지 초점을 맞춰서 프로젝트를 회고해보고자 합니다. 주 포인트는 아래와 같습니다.</p>
<blockquote>
  <ol>
    <li>유의차 분석</li>
    <li>데이터 셋이 불균형일 때 해결방법</li>
  </ol>
</blockquote>

<h2 id="1-프로젝트-개요">1. 프로젝트 개요</h2>
<p>  이번 프로젝트에서 클라이언트의 요청사항은 2가지 였다. 첫 번째는 특정 공정에서 발생하는 품질 이상 발생 방지를 위하여 핵심인자를 규명하는 것이었고, 두 번쨰로는 향후 
생산속도 증속 대비 최적 공정조건을 설정하기 위한 시뮬레이션을 위한 품질 이상 발생 확률을 예측하는 예측모형 개발이었다.</p>

<h3 id="11-문제-정의">1.1 문제 정의</h3>
<p>  우리는 문제를 풀기 위해 Y인자를 품질 이상 발생/미발생 제품을 나타내는 이항 변수(Binary Variable)로 정의했다. 당시 클라이언트는 제품별로 품질 이상 발생 이력을 데이터화하여 관리하고 있었기에
해당 데이터를 Y인자로 활용했고, 관련된 모든 데이터를 모아 분석을 위한 데이터 마트를 구성했다.</p>

<h3 id="12-가설-설정--분석-전략-수립">1.2 가설 설정 &amp; 분석 전략 수립</h3>
<p>  품질 이상 발생 여부를 기준으로 우리는 제품을 두 그룹으로 나눴고, 유의차 분석 방법을 적용해 핵심인자를 규명하기로 했다. 우리의 가설은 품질에 영향을 미치는 핵심인자라면,
 양 그룹 간 평균 혹은 분포 등 차이가 있을 것이라는 점이었다. 우리는 탐색적 자료 분석의 단계를 아래와 같이 두 단계로 놔눴고, 하나하나 잠재 인자들을 탐색하기로 했다.</p>
<blockquote>
  <ol>
    <li>t-test 검정을 통해 양 그룹 간 평균의 차이가 있는 인자들을 탐색한 후,</li>
    <li>라인차트, 산점도, 박스플롯 등을 활용하여 후보인자의 그룹 간 차이에 대해서 엄밀하게 검증함</li>
  </ol>
</blockquote>

<p>통계 검정은 만능이 아니기에 통계 검정만으로 결론을 내기에는 부족하고 2번의 과정 역시 매우 중요하다. 실제로 분석하다보면, 통계적 검정이 통계적으로 유의미한 결과를 보이는 경우에도 데이터를 시각적으로 확인해보면 
아닌 경우가 매우 많다.</p>

<h3 id="13-예측-모델링">1.3 예측 모델링</h3>
<p>  설명 가능한 예측모형을 고려하여 우리는 로짓회귀 모형을 선택해 예측 모형을 개발하기로 했다. 유의차 분석 결과를 통해 도출한 핵심인자와 기본 생산 조건을 나타내는 변수들을 조합하여 회귀식을 세웠다. 보통 Y인자가 이항 변수일 때, 예측 모형 평가지표는 정확도, AUC 등을 활용한다. 하지만 우리의 목표는 불량 제품을 제대로 예측하는 것이었기에 불량 정확도 역시 중요 평가지표로 활용해 예측 모형의 예측력을 평가했다.</p>

<p>예측 모형을 개발하기 위해 풀어야 했던 문제 중 하나는 데이터 셋의 불균형이었다. 당시 데이터 셋에서 불량 제품의 비중이 5%도 안될 정도로 불균형이 심각했다.
원자료셋을 그대로 사용했을 경우 예측 모형의 예측력이 좋지 않았기에 예측력을 향상시키기 위해서 고민했고, 샘플링 기법을 활용해 예측력을 보완하고자 했다.</p>

<h3 id="14-결과">1.4 결과</h3>
<p>  샘플링 기법을 활용해 다양한 데이터 셋을 만들어서 학습하여 예측력을 비교해 최종 모형을 선택했고, 주요 인자의 값에 따라 불량 제품 확률을 시뮬레이션하여
클라이언트에게 공유했다. 솔직하게 말하자면 ML모형의 예측력은 아주 떨어지는 것은 아니었으나 예측 결과에 대한 클라이언트의 결론은 실제 현장에 적용하기 어렵다는 것이었다. 이유는 예측 모형의 포함된 한 변수에 대해서 계수의 부호가 반대로 나왔기 때문이다. 참고용으로만 활용됐고 ML모형을 보완하거나 후속 작업이 이뤄지진 않았다.
어떠한 조건에서 불량 제품이 발생할 확률이 높아지는지 확인할 수 있어서 소기의 성과를 올렸기에 해당 프로젝트는 종료하고 다음 프로젝트로 넘어갔다.
<br /></p>

<h2 id="2-새로-배운-것">2. 새로 배운 것</h2>

<p>  이번 프로젝트는 주니어 데이터 분석가로서 많은 것을 배울 수 있던 프로젝트였다. 특히, 불균형 데이터 셋 하에서 유의차 분석과 예측 모형 개발에 대해서
심도있게 고민해볼 수 있는 좋은 기회였다.</p>

<h3 id="21-불균형-데이터-셋">2.1 불균형 데이터 셋</h3>
<p>  학교에서 공부할 때에는 불균형이 심한 데이터 셋을 다뤘던 경험이 없었기에 이번 프로젝트가 처음으로 불균형햔 데이터 셋을 다룬 첫 번째 프로젝트였다.
당시 데이터 셋에서 정상/불량 제품이 대략 900/15 정도 였던 걸로 기억한다(정확하지 않다). 분석을 진행해보니 데이터 셋의 불균형이 심할 경우, 2가지 관점에서
문제가 발생했다.</p>
<blockquote>
  <ol>
    <li>t-test 검정 결과의 신뢰도 하락</li>
    <li>ML모형의 예측력 하락</li>
  </ol>
</blockquote>

<h4 id="211-t-test-검정">2.1.1 t-test 검정</h4>
<p>  불균형이 심할 때, 인자의 양 그룹 별 평균 차이를 검정하면 어떻게 될까!? 대부분의 인자의 t-test 검정에서 5% 유의수준에서 귀무가설을 기각하는 결과가 나왔다. 즉, 웬만하면 통계적으로 양 그룹 간 평균의 차이가 있다는 것이다. 따라서 t-test 검정을 통한 유의차 분석 만으로는 유의미한 결론을 도출하기 어려웠다. <br /><br />
왜 그럴까? 다음과 같은 간단한 실험을 해보자.
<br /><br />
  우선 실험을 위해 임의의 100개의 X인자 데이터를 만들었다. 0 ~ 1 범위 내에서 10개의 임의의 난수를 생성해 불량 그룹 데이터를 만들고, 마찬가지로 0 ~ 1 범위 내에서 35개와 1 ~ 2 범위 내에서 45개 총 90개의 임의의 난수를 생성해 정상 그룹 데이터를 생성했다. 그룹 별 데이터의 평균값은 다음과 같다.</p>

<table>
  <thead>
    <tr>
      <th>그룹</th>
      <th>평균</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>불량</td>
      <td>0.33</td>
    </tr>
    <tr>
      <td>정상</td>
      <td>1.05</td>
    </tr>
  </tbody>
</table>

<p>  그룹 간 X인자의 평균 차이는 대략 0.7이며, 평균 차이에 대한 t-test 검정 결과 p-value = 0.0002로 통계적으로 매우 유의미한 평균 차이가 존재한다는 결과가 나왔다. t-test 검정 결과만 보면, 이 X인자의 값이 낮을 때, 불량 제품 유발 확률이 높다고 보여진다. 하지만 정말 이 수치만을 신뢰하여 이런 결론을 도출해도 될까? 이 데이터를 시각적으로 간단하게 표현해봤다.</p>
<div>
<div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script>                <div id="8a5d5aeb-666d-43c6-bddb-cfecc9b7a61b" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("8a5d5aeb-666d-43c6-bddb-cfecc9b7a61b")) {                    Plotly.newPlot(                        "8a5d5aeb-666d-43c6-bddb-cfecc9b7a61b",                        [{"line":{"color":"Black"},"marker":{"color":["Red","Red","Red","Red","Red","Red","Red","Red","Red","Red","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black","Black"],"size":7},"mode":"markers+lines","y":[0.7769972087333984,0.03889458647881372,0.3004485525296108,0.16699236232276138,0.05203445491861958,0.21649268718373893,0.43530264017478626,0.34467776112017257,0.9529859443621004,0.023662017175597905,0.02352598802920358,0.49711877206667,0.9495350328222201,0.71466426046685,0.4138630030531898,0.9408966488165194,0.43670069651096777,0.8245807411458221,0.021426893598750163,0.332720369444257,0.8879789022248927,0.45978201700441024,0.9768309041541778,0.2448783532988419,0.9485246953142914,0.01894543462659881,0.9047230442115333,0.14999139242994297,0.3499903322849244,0.9300748108354427,0.38498672318353566,0.6650391671013915,0.546041680952944,0.01716128760878788,0.5741588485610468,0.38503171774243816,0.10795776321811135,0.8494159561746308,0.8308301116208272,0.1652411342691531,0.8989583646025601,0.31194638083688375,0.27156866813311864,0.2689178781893652,0.2193423379067042,1.1784748763817126,1.7658031939332388,1.3481453406165258,1.304484945037001,1.793576940737211,1.689500912271365,1.2845845143502719,1.6264501217613794,1.3321124334668832,1.729561270316494,1.9421555944722193,1.9103507092406877,1.0369298023145088,1.2125590215503559,1.6089974358346342,1.6572880526174565,1.1902944917472444,1.3406997429013634,1.6344212759665466,1.1582354892980717,1.6016150119625077,1.9614311110523093,1.0689093437947874,1.1481630902033042,1.0562050675392238,1.72134853536979,1.0325313269100111,1.5621629032170308,1.267631348920546,1.8415591700054201,1.9532417415025956,1.7370590079480488,1.3010980404234886,1.4491012361321691,1.7309471463506303,1.8068695738317069,1.341858144151714,1.6811880891237838,1.5456789690516115,1.2092383294302593,1.7743080585402435,1.4053434066087196,1.6676957599726534,1.03508996436525,1.5399983789541616],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"title":{"font":{"color":"black","size":20},"text":"\ubd88\ub7c9 = Red/\uc815\uc0c1 = Black"},"plot_bgcolor":"white","xaxis":{"showgrid":true,"gridwidth":0.1,"gridcolor":"#eee"},"yaxis":{"showgrid":true,"gridwidth":0.1,"gridcolor":"#eee"}},                        {"responsive": true}                    )                };                            </script>        </div>
</div>
<p><br />
  그래프를 확인해보면 전혀 그렇지 않다는 것을 알 수 있다. 이 X인자의 변동만으로는 불량 제품 발생에 대해서 전혀 설명하지 못하고 있다. 물론 실험을 위해 임의로 만든 데이터 셋이긴 하지만 프로젝트를 진행하면서 실제로 자주 마주쳤던 패턴이다. 제대로 교육받고 훈련된 데이터 분석가라면 t-test 검정 결과가 만능이 아니라는 것은 누구나 다 알 것이다. 하지만 수백 ~ 수천개의 인자들을 하나하나 시각적으로 탐색하기에는 프로젝트 수행기간이 항상 넉넉하지 않기에 이런 통계 검정, 통계 수치에 의존하여 인자들을 필터링하며 데이터 분석을 진행하게 된다. 중요한 점은 통계 검정 수치만 전적으로 신뢰하는 것이 아니라 다양한 방면으로 분석하고 검증해야 한다는 것이다.</p>

<h4 id="212-샘플링">2.1.2 샘플링</h4>
<p>  불균형 데이터 셋을 학습했을 때 ML모형의 예측력이 좋지 않다는 것은 이미 널리 알려진 사실이지만 실제로 문제를 인식하고 배울 수 있던 것은 이번 프로젝트에서였다. 불균형 문제를 해결하기 위해서 팀장님은 랜덤 샘플링을 통한 해결 방법을 제시했고 이에 더하여 또 좋은 것들이 있는지 탐색했고 다음 방법을 통해 불균형을 해결하고자 했다.</p>
<blockquote>
  <ol>
    <li>Random Sampling[Over, Under, Hybrid(Over + Under)]</li>
    <li>SMOTE Algorithm</li>
  </ol>
</blockquote>

<p>  첫 번째 방법은 랜덤 샘플링이다. 랜덤 복원추출을 통해 크기가 큰 데이터 그룹의 수는 줄이고, 적은 그룹의 수는 늘려 불균형을 보정하는 방식이다. 장점은 적용하기 쉽지만, 단점은 크기가 작은 데이터 그룹의 데이터가 단순 복제된다는 점이다. SMOTE 알고리즘은 그룹 내에 데이터 간의 선형 결합을 통해 임의의 데이터를 생성하는 방식이다. 단순 복제하는 랜던 샘플링 방식에 비해 새로운 데이터가 생성된다는 점과 이해하기 쉽다는 장점이 있지만, 반대로 가상의 데이터를 임의로 생성한다는 단점이 있다. 하지만, 데이터를 생성하는 방식이 받아들일만 하다고 생각했고 적용하여 예측력을 검토해보고자 했다. <br />
  실제로 각 방법을 적용하여 불균형을 보정하고 ML모형의 예측력을 비교했을때, Random Sampling(Hybrid)와 SMOTE 알고리즘을 적용한 데이터 셋의 예측력이 향상된 것을 확인할 수 있었다. 단순한 방법으로도 ML모형의 예측력이 증가할 수 있다는 점이 흥미로웠다. ML모형이 데이터를 학습하는 것을 생각해보면 충분히 납득할만하다고 생각한다. 데이터 셋의 불균형이 심하다면 ML모형이 데이터의 수가 많은 그룹을 위주로 학습하기 때문이다. (이 말이 이론적으로 엄밀히 맞는 말인지는 모르겠다.) 프로젝트 중에는 시도해보지는 못했지만 회고록을 쓰고 있는 지금 시점에서 생각해보니 더 디테일하게 샘플링 방법을 설정하여 적용해보고 결과를 비교하는 것도 재밌는 작업이 될 것 같다. <br />
  한가지 조심해야 할 점은 과도한 보정이 들어갈 경우, 이 작업이 데이터 조작이 될 수도 있다는 점이다. 랜덤 샘플링의 경우 문제가 덜 하겠지만, 가상의 데이터를 생성하는 알고리즘(예: SMOTE)을 적용할 때는 항상 조작이 되지 않도록 유의하여 적용해야 할 것이다.</p>

<h2 id="3-아쉬운-점">3. 아쉬운 점</h2>
<p>  개인적으로 아쉬웠던 점은 고생해서 개발한 ML예측 모형이 클라이언트에게 큰 도움이 되지 않았다는 점이다. 당시, 클라이언트가 준 피드백 중 하나는 <strong>“특정 인자의 부호가 이론적으로 맞지 않는다”</strong> 였다. 하지만 그 인자는 중요한 외생 조건 변수라 회귀식에서 제거할 수 없었다. 그리고, 부호가 반대로 나온 이유도 사실 명확했다. (문제가 된 변수를 A변수라 해보자)</p>
<blockquote>
  <p>A인자는 시간이 지남에 따라 계속 <strong>증가</strong>해왔고, 불량률은 시간이 지남에 따라 지속적으로 <strong>감소</strong>해왔다.</p>
</blockquote>

<p>  시간이 지남에 따라 위 두 변수가 맞물려 마치 A인자가 증가하면 불량률이 감소할 것이라는 결과가 도출된 것이다. 데이터의 수가 충분했다면 A인자의 값에 따라 데이터 셋을 나누면(층별화) 될 것이다. 하지만 그 당시, 데이터가 충분하지 않았기에 데이터 셋을 더 작게 나눌 수 없는 한계점이 존재했다. 클라이언트는 핵심인자과 그 매커니즘을 규정한 것으로도 만족했고, 이 결과를 활용해 충분히 의사결정이 가능하다고 하여 추가 보완을 진행하지 않았다. 필요하다면 추후 데이터의 수가 충분해졌을 때, 다시 시도해보고자 하고 이번 프로젝트는 종료됐다.</p>

<h2 id="4-마무리하며">4. 마무리하며</h2>
<p>  이번 프로젝트는 실무에서 처음으로 유의차 분석과 예측 모형을 개발하여 적용한 프로젝트였기에 배운게 많은 프로젝트로 기억에 남는다. 개인적으로는 로짓 모형 말고 다른 분류 모형을 학습했을 때와 데이터의 수가 훨씬 많았다면 결과가 어떻게 달라졌을까 하는 궁금증도 많이 남는 프로젝트였다.</p>]]></content><author><name>박마토</name></author><category term="Data" /><category term="Data" /><category term="Projects" /><category term="Memoir" /><summary type="html"><![CDATA[  안녕하세요. 이번에는 Y인자가 이항 변수(Binary Variable)일 때, 어떻게 문제에 접근했고 풀었는지 초점을 맞춰서 프로젝트를 회고해보고자 합니다. 주 포인트는 아래와 같습니다. 유의차 분석 데이터 셋이 불균형일 때 해결방법]]></summary></entry><entry><title type="html">[프로젝트회고록] 01. 첫번째 분석 프로젝트: 올바른 문제정의와 가설 설정의 중요성</title><link href="http://localhost:4000/data/projects_memoir1/" rel="alternate" type="text/html" title="[프로젝트회고록] 01. 첫번째 분석 프로젝트: 올바른 문제정의와 가설 설정의 중요성" /><published>2023-03-02T00:00:00+09:00</published><updated>2023-03-03T01:43:00+09:00</updated><id>http://localhost:4000/data/projects_memoir1</id><content type="html" xml:base="http://localhost:4000/data/projects_memoir1/"><![CDATA[<blockquote>
  <p>  회고록 시리즈는 수행했던 프로젝트를 정리하고 기록하기 위한 시리즈입니다. 자세한 내용은 회사의 기밀 유지를 위하여 오픈하기 어렵지만, 문제에 어떻게 접근했고, 문제를 어떻게 해결했고, 어떤 인사이트를 도출했고, 아쉬운 점 등 쓸 수 있는 만큼 정리하겠습니다.</p>
</blockquote>

<p>  ‘20년 9월 입사한 후 짧은 적응기간을 걸치고 11월말 킥오프 미팅을 시작으로 바로 프로젝트에 투입됐다. 신입인데 프로젝트에 바로 투입되어 설렘 반 걱정 반으로 시작했던 것 같다. 결론부터 말하자면, 데이터 분석을 통해 의미있는 결과를 도출해내어 뿌듯했지만, 정말 많은 시행착오를 겪었고 더 잘할 수 있었을텐데 하는 아쉬움을 남긴 프로젝트였다.</p>
<h2 id="1-프로젝트-과정">1. 프로젝트 과정</h2>
<p>  우리에게 주어진 문제는 공정 이상 발생 방지를 위한 핵심인자를 규명하고 최적 조건을 탐색하는 것이었다. 공장에서 오랫동안 풀지 못했던 문제였고 공정 이상 발생 시, 작게는 가동률 하락과 심각하게는 설비 고장으로 인하여 공정 라인이 몇개월 간 중단될 수 있을 만큼 중요한 사안이었다.</p>

<h3 id="11-문제-정의">1.1 문제 정의</h3>
<p>  우리는 Y인자가 특정 관리 기준을 넘어갔을 때를 공정 이상이라고 정의하였고, 공장 엔지니어와의 협업을 통해 공정 이상 발생 CASE의 특징을 탐색해 다음과 같이 2가지 케이스로 정리할 수 있었다.</p>
<blockquote>
  <ol>
    <li>어떠한 이유로 생산속도가 저속에서 고속으로 증속할 때 종종 공정 이상이 발생</li>
    <li>특정 제품군 생산 시, 중간 공정에서 수작업 진행 시 공정 이상 발생</li>
  </ol>
</blockquote>

<p>공정 이상 CASE의 특징을 정의하고 나니 분석 대상이 명확해졌다. 우리는 2가지 CASE에 맞게 공정 EVENT들을 공정 이상 발생/미발생 그룹으로 나눴다.</p>

<h3 id="12-가설-설정">1.2 가설 설정</h3>
<p>  우리의 대전제가 된 가설은 공정 이상 발생/미발생 그룹 간 Y인자에 영향을 미치는 X인자의 패턴의 차이가 있다는 것이었다. 이에 더하여 현업 엔지니어가 공정 메커니즘에 기반하여 몇가지 구체적인 가설을 설정했다. 분석을 진행하면서 가설을 검증하고 수정하고 새로운 가설을 설정하는 과정의 반복이었던 것 같다.</p>

<h3 id="13-검증분석">1.3 검증(분석)</h3>
<p>  분석 초기에는 맨 땅에 해딩하듯이 X인자 하나하나 그룹 간 패턴 비교 분석을 통해 핵심인자인지 아닌지를 검증해나갔다. 문제는 세부적인 가설 설정없이 무식하게 분석을 진행하다보니 인사이트를 찾지 못하고 애꿎은 시간만 점점 흐르고 있었다. 결국 팀장님과 나는 분석 프로세스를 원점에서부터 재검토했다.</p>
<blockquote>
  <ul>
    <li><strong>공정 이상 발생 그룹 재정의</strong>: Event의 Noise Case를 제거하여 그룹을 명확하게 분류함. 현업에서 제공한 공정 Event 리스트의 라벨링이 명확하지 않았기 때문</li>
    <li>그룹 재정의 후, 더욱 <strong>디테일하게 비교 대상을 선정함</strong>: 제품군 별, 공정 상황 별 각각 Case들을 재분류하였고 최대한 외적인 요인들을 통제하고자 함</li>
  </ul>
</blockquote>

<p>그룹을 재정의하고 비교 대상을 명확히 하니, 이전에는 보이지 않던 그룹 간 주요 X인자의 패턴 차이가 명확하게 보였다. 이 패턴 차이를 근거로 새로운 가설을 설정했다.</p>
<blockquote>
  <ul>
    <li>주요 X인자의 패턴 차이를 유발한 인자가 분명히 존재할 것임</li>
    <li>앞서, CASE의 특징 중 하나인 <strong>증속 과정</strong> 중에 발생하는 것에 집중하여 속도 변화 과정 중에 패턴을 분석하기로 함</li>
  </ul>
</blockquote>

<p>이젠, 모호했던 것들이 모두 정리가 되어 명확해졌고 더 이상 분석 방향을 잡지 못해 해맬 일이 없었다. 우리는 새롭게 설립한 가설에 대해 검증하기 위해 X축의 “생산속도” 그리고 Y축의 “후보인자”를 그려가면서 속도 별 후보 인자의 패턴을 그렸고, 결국
특정 조건을 나타내는 인자들에 대해서 그룹 간 특징이 명확하게 대비되는 것을 발견할 수 있었다.</p>

<h3 id="14-결과">1.4 결과</h3>
<p>  우리는 도출해낸 인사이트를 엔지니어에게 공유했고, 세부적인 추가 분석을 거쳐 우리의 의견을 받아들였다. 이제 우리와 엔지니어들은 과거 Best Practice 데이터 기반으로 새로운 관리 기준을 정립했다. 물론, 이 뒷 부분은 분석가의 역할이 아니라 엔지니어가 주된 역할을 맡아 진행했다. 엔지니어들은 새로운 증속 과정 중 새로운 관리 기준을 적용하여 운전했고, Human Error를 제외하고는 더 이상 문제가 발생하지 않는 결과를 얻을 수 있었다.</p>

<h2 id="2-느낀점">2. 느낀점</h2>
<p>  첫 프로젝트이다보니 정말 해매기도 많이 해맸고 고생도 많이 했던 프로젝트였다. 매주 2박 3일 진행되틑 출장 일정부터 어지러운 공정 용어들까지 하나하나 쉬운 것들이 없었다. 분석 외에도 느낀 것들이 많은데 정리하자면 다음과 같다.(아쉬웠던 점들은 따로 챕터를 분리했다.)</p>

<h3 id="21-커뮤니케이션">2.1 커뮤니케이션</h3>
<p>  당시에는 첫 프로젝트이다보니 도메인 영역에 대한 지식이 약한 분석가팀과 데이터 분석에 대한 이해도가 낮은 엔지니어팀 간의 커뮤니케이션이 개인적으로 정말 어려웠다. 특히, 우리팀의 상사와 고객 사이에서 중간 다리 역할을 해야하는 나로서는 매 순간순간이 어려웠었는데, 아 다르고 어 다르다고 내용을 잘못 전달하게 되면 분석 내용에 대해 오해가 빚어질 수 밖에 없는 상황이었기 때문이다. 그리고 엔지니어 분들의 공정에 대한 자부심이 대단했기에 잘못된 지식에 대해서 지적하거나 새로운 사실에 대해서 긍정적으로 받아들이는 분위기가 아니였다. 그런 엔지니어 분들과 소통하기 위해서는 명확한 논리와 데이터 기반의 근거가 뒷받침되어야 한다는 것을 배울 수 있었다.</p>

<h3 id="22-데이터-분석">2.2 데이터 분석</h3>
<p>  학교에서 과제를 하거나 석사 논문 작성을 위한 연구를 할 때에도 머리를 쥐어 뜯어가며 했었는데, 실무라고 다를 건 없었다. 입사하고 나서 적응기간 중 팀장님이 입버릇처럼 말씀하셨던 것이 있는데</p>
<blockquote>
  <p>“데이터 분석 별거 아니다. 더 많이 보고, 더 깊게 고민하고 생각하다보면 풀린다.”</p>
</blockquote>

<p>이다. 실제로 해보니 맞는 말이었다. 분석가로 입사하기 전에는 화려한 분석기법과 테크닉, 코딩 등에 대한 환상이 있었는데 그런것보다는 결국 문제를 풀기 위해 데이터를 더 많이 들여다보고 더 고민하고 생각했는지에 대한 싸움이었다. 데이터 분석 과정은 생각보다 멋있지 않고 지리하고 괴로운 고통의 과정이었다. 하지만, 문제를 풀어나가는 그 자체가 재밌었고 해결했을때의 그 성취감과 뿌듯함은 쉽게 잊지 못할 것이다.</p>

<h3 id="23-데이터의-중요성-인식">2.3 데이터의 중요성 인식</h3>
<p>  데이터가 중요하다고 알고만 있는 것과 실제로 데이터를 통해 인사이트를 도출하고 적용을 통해 개선되는 사례를 직접 접하는 것은 다르다. 난 이제 어디가서도 데이터의 중요성과 올바르게 활용할 수 있다면 데이터가 어떤 가치를 지니고 있는지 당당하게 말할 수 있게 됐다.</p>

<h2 id="3-아쉬웠던-점과-개선-포인트">3. 아쉬웠던 점과 개선 포인트</h2>
<h3 id="31-올바른-문제-정의와-가설-설정의-중요성을-간과함">3.1 올바른 문제 정의와 가설 설정의 중요성을 간과함</h3>
<p>  분석 프로젝트에 착수하여 분석을 기획하게 되면 반드시 문제에 대한 정의와 가설 설정해야 하는 절차가 필요하고 굉장히 중요하다는 것을 느꼈다. 위에도 서술했듯이 위 과정을 제대로 거치지 않아 분석 방향을 잃었고 많은 시간을 낭비하게 됐다. 그러다보면 프로젝트 수행 기간이 늘어질 뿐 아니라 분석 퍼포먼스도 떨어지며 결국은 의미있는 결과를 도출하기 어려울 것이다. 앞으로는 올바른 문제 정의와 가설 설정을 위해 다음과 같은 점들에 유의하며 프로젝트를 진행하고 있다.</p>
<blockquote>
  <ul>
    <li>문제 정의와 가설 설정 철저하게: 시간 없다고, 바쁘다는 핑계로 대충하지 않기, 그리고 반드시 본인 외에 다른 사람에게 공유하고 확인하기</li>
    <li>도메인 지식에 대한 철저한 이해: 이해가 안된 것들은 대충 뭉개고 넘어가지 않기</li>
    <li>적극적인 커뮤니케이션: 진행 내용에 대해 관련 담당자에게 공유하고 Feedback 받기, 질문하는 것을 어려워하지 않기</li>
    <li>데이터 분석가의 시각으로 접근해보기: 도메인 전문가의 말이 100% 맞는 것은 아님, 충분히 의심해볼만한 것들은 의심해보기</li>
  </ul>
</blockquote>

<h2 id="4-맺음말">4. 맺음말</h2>
<p>  회고록을 정리하다보니 2년 전 첫 프로젝트를 수행하던 당시의 기록이 새록새록 떠오른다. 프로젝트를 거듭하다보니 문제 정의와 가설 설정의 중요성이 점점 더 크게 느껴지고 있는 요즘이다. 귀찮다고, 급하다고, 그냥, 귀찮으니깐, 문제 정의와 가설 설정을 대충 하지 말자는 다짐과 함께 회고록을 여기서 이만 마무리 하겠습니다. 긴 글 읽어주셔서 감사합니다.</p>]]></content><author><name>박마토</name></author><category term="Data" /><category term="Data" /><category term="Projects" /><category term="Memoir" /><summary type="html"><![CDATA[  회고록 시리즈는 수행했던 프로젝트를 정리하고 기록하기 위한 시리즈입니다. 자세한 내용은 회사의 기밀 유지를 위하여 오픈하기 어렵지만, 문제에 어떻게 접근했고, 문제를 어떻게 해결했고, 어떤 인사이트를 도출했고, 아쉬운 점 등 쓸 수 있는 만큼 정리하겠습니다.]]></summary></entry><entry><title type="html">[하루일기] ‘23/02/23 - 엑셀도 DB일 수 있을까?!(ft. 데이터 전처리)</title><link href="http://localhost:4000/data/daily-memo/" rel="alternate" type="text/html" title="[하루일기] ‘23/02/23 - 엑셀도 DB일 수 있을까?!(ft. 데이터 전처리)" /><published>2023-02-23T00:00:00+09:00</published><updated>2023-02-24T08:39:00+09:00</updated><id>http://localhost:4000/data/daily-memo</id><content type="html" xml:base="http://localhost:4000/data/daily-memo/"><![CDATA[<p>안녕하세요.<br /><br />
데이터 분석 실무를 하다보니 어느새부턴가 개인적으로 엑셀로 저장된 데이터를 기피하게 되더군요. 이번 분석 프로젝트에서 클라이언트사의 중요한 데이터가 엑셀로 보관되어 있었고,
최근 5년치의 데이터를 엑셀로부터 가져와야 했습니다. 데이터 수집 프로그램을 짜다가 발생하는 수많은 오류를 마주하면서 얼마 전 팀장님이 고객에게 지나가듯이 했던 말이 떠올랐습니다.</p>
<blockquote>
  <p>“엑셀도 DB의 일종이죠. “<br /></p>
</blockquote>

<p>정말로 엑셀도 DB일 수 있을까? 곰곰이 곱씹어보다가 엑셀이 DB의 일종일 수 있으나 DB로서의 기능을 완벽하게 구현하기 어렵다 생각했습니다. 사실 엄격하게 보자면 DB라고 보기 어렵다고 생각합니다.
제 나름대로 그 이유를 생각해봤습니다. <br /></p>
<ol>
  <li><strong>수기 입력</strong>
    <ul>
      <li>Human Error로 인하여 오타 혹은 데이터가 누락될 확률이 높음</li>
      <li>데이터 입력 제한/제약을 가할 수 있는 기능 부재</li>
      <li>분석가 입장에서 데이터를 전적으로 신뢰하기 어려움</li>
    </ul>
  </li>
  <li><strong>데이터 수집 속도</strong>
    <ul>
      <li>DB와 비교했을 때, 데이터 처리 속도가 매우 떨어짐</li>
    </ul>
  </li>
  <li><strong>안정적인 운영 담보 어려움</strong>
    <ul>
      <li>엑셀 데이터를 지속적으로 활용한다 했을 때, 위 1번의 문제가 없으리라고 장담할 수 없음</li>
    </ul>
  </li>
</ol>

<p>제가 요 몇일동안 했던 작업도 데이터가 엑셀이 아닌 DB에 있었다면 1시간도 안되서 끝났을 일이라 생각합니다. 엑셀로부터 가져오다보니 수많은 오류를 일일이 프로그래밍을 통해 대응해야 했고,
속도는 더디고 하다보니 길어지네요. 제가 모든 발생 가능한 에러에 대해 미리 대응할 수 있었다면 좋았겠지만, 아직 그 정도 수준은 무리인 것 같습니다. <br /><br />
그럼에도 왜 엑셀을 사용할까요? 이것도 지극히 개인적인 견해이지만 풀어보겠습니다.</p>

<ol>
  <li>관성
    <ul>
      <li>예전부터 써왔으니 편하니 별 생각없이 씀</li>
      <li>안그래도 본업에 치이는데 초기 시스템 구축을 위하여 짬을 내기 힘듬</li>
    </ul>
  </li>
  <li>비용
    <ul>
      <li>기업 입장에서는 시스템 구축을 위해서 비용이 투자되어야 하지만 투자 대비 정량적인 성과가 뚜렷하지 않음</li>
      <li>실무자, 관리자 모두 필요성에는 공감하나 비용 문제로 결정권자에 승인이 떨어지지 않는 경우</li>
    </ul>
  </li>
  <li>역량
    <ul>
      <li>노후화된 조직일수록 새로운 시스템을 받아들이기 힘들어 함</li>
      <li>리더가 의지를 갖고 조직원을 설득하고 적극적으로 밀어붙여야함</li>
    </ul>
  </li>
</ol>

<p>쓰다보니 엑셀에 대해서 너무 안좋게만 쓴 것 같네요. 무조건 나쁘다는 것은 아닙니다. 엑셀은 다양한 기능을 제공하며 사용하기 편한 장점이 있습니다. 적절하게 활용한다면 더할나위 없는 툴이겠죠.
하지만, 이제는 엑셀 데이터는 그만… 노노… 물론 분석가로서 데이터 수집과 전처리도 척척 해내야하지만, 좋은 것을 놔두고 굳이 돌아가야할 필요가 있을까요?<br />
앞으로도 당분간은 시스템화 하지 않고 임시방편으로 특정 서버에 올려놓은 엑셀 데이터를 처리해서 가져오기로 했는데, 앞날이 뻔히 보이는 건 저만의 착각일까요? 잘 되겠죠ㅎㅎ</p>]]></content><author><name>박마토</name></author><category term="Data" /><category term="Data" /><category term="DB" /><summary type="html"><![CDATA[안녕하세요. 데이터 분석 실무를 하다보니 어느새부턴가 개인적으로 엑셀로 저장된 데이터를 기피하게 되더군요. 이번 분석 프로젝트에서 클라이언트사의 중요한 데이터가 엑셀로 보관되어 있었고, 최근 5년치의 데이터를 엑셀로부터 가져와야 했습니다. 데이터 수집 프로그램을 짜다가 발생하는 수많은 오류를 마주하면서 얼마 전 팀장님이 고객에게 지나가듯이 했던 말이 떠올랐습니다. “엑셀도 DB의 일종이죠. “]]></summary></entry><entry><title type="html">[DB] 데이터베이스(DB)란?!</title><link href="http://localhost:4000/data/db_1/" rel="alternate" type="text/html" title="[DB] 데이터베이스(DB)란?!" /><published>2023-02-09T00:00:00+09:00</published><updated>2023-02-07T06:39:00+09:00</updated><id>http://localhost:4000/data/db_1</id><content type="html" xml:base="http://localhost:4000/data/db_1/"><![CDATA[<p>안녕하세요. <br /><br />
요즘 DB와 SQL 공부를 하고 있는데요. 데이터 분석가로 일하다보니 DB와 SQL에 대해서 제대로 알아야 겠다는 생각이 드는 요즘입니다. 왜냐하면 대부분의 데이터들은 보통 데이터베이스에 보관되어 있고 우리는 그 데이터베이스에 접근하여 데이터를 가져와야 하기 때문이죠. 따라서 최근 토이프로젝트로 가계부 프로그램을 만들어보면서 DB를 직접 설계해보고 공부한 것들 그리고 SQL에 대해서도 한 번 정리해보고자 합니다.</p>

<h2 id="1-데이터베이스란">1. 데이터베이스란?</h2>
<p>소위 말하는 데이터베이스 혹은 DB란 무엇일까요? 쉽게 말하면 데이터가 보관되어 있는 보관소라고 보면 될 것 같습니다. 조금 더 개념을 확장하자면 다수의 사용자들이 사용할 수 있도록 다량의 데이터들을 모으고 관리할 수 있도록 한 것이 데이터베이스입니다.</p>
<h3 id="11-데이터베이스의-정의">1.1 데이터베이스의 정의</h3>
<ul>
  <li>통합 데이터(Integrated Data): 데이터의 <strong><u>중복을 제거</u></strong> 한 데이터 집합</li>
  <li>저장 데이터(Stored Data): <strong><u>컴퓨터가 접근 가능한 저장장치</u></strong>에 저장된 데이터 집합</li>
  <li>운영 데이터(Operational Data): 조직 업무(운영)에 반드시 필요한 데이터 집합</li>
  <li>공유 데이터(Shared Data): 여러 응용 시스템에서 <strong><u>공동으로 사용 가능</u></strong>한 데이터 집합</li>
</ul>

<h3 id="12-데이터베이스의-특징">1.2 데이터베이스의 특징</h3>
<p>사용자가 데이터를 데이터베이스에 등록, 정리, 검색이 가능한데, 그 중 데이터베이스가 갖는 특징은 다음과 같습니다.</p>
<ul>
  <li>실시간 접근성(Real Time Accessibility): 사용자가 질의하는 내용에 대해 실시간 처리에 의한 응답이 가능</li>
  <li>동시 공유(Concurrent Sharing): 다수의 이용자가 데이터베이스 이용 가능</li>
  <li>내용에 의한 참조(Content Reference): 데이터의 물리적 위치가 아닌 데이터 값에 따라 참조됨</li>
  <li>계속적인 진화(Continuous Evolution): 데이터베이스의 상태는 동적이며, 지속적으로 변화함</li>
</ul>

<h2 id="2-dbmsdatabase-management-system-데이터베이스-관리-시스템">2. DBMS(DataBase Management System): 데이터베이스 관리 시스템</h2>
<p>데이터베이스는 데이터가 저장되어 있는 보관소이며, 그 자체로는 특별한 기능을 갖지 않습니다.
따라서, 우리가 데이터베이스 내의 데이터에 접근하여 활용하기 위해서는 DB를 조작할 수 있는 특정 소프트웨어가 필요한데, 이를 바로 <strong>DBMS(데이터베이스 관리 시스템)</strong>라고 합니다.
<br />
우리가 DBMS에 명령을 전송하면, DBMS는 명령에 따라 데이터베이스를 처리합니다. 즉, 사용자와 데이터베이스 사이에 중개자 역할을 한다고 이해하셔도 될 것 같습니다.</p>

<h3 id="21-dbms-기능">2.1 DBMS 기능</h3>
<p>DBMS의 필수 기능으로는 정의(Definition), 조작(Manipulation), 제어(Control)가 있다.</p>

<ul>
  <li>정의(Definition)
    <ul>
      <li>데이터베이스의 저장될 데이터의 형(Type)과 구조에 대한 정의, 이용방식, 제약 조건 등을 명시</li>
      <li>데이터와 데이터 간 관계를 명확하게 명시함</li>
    </ul>
  </li>
  <li>조작(Manipulation)
    <ul>
      <li>데이터의 검색, 수정, 삽입, 삭제, 정렬 등을 처리하기 위한 기능</li>
    </ul>
  </li>
  <li>제어(Control)
    <ul>
      <li>데이터의 무결성 유지</li>
      <li>접근 권한 관리를 통한 보안 유지 및 권한 검사</li>
      <li>데이터의 일관성 유지</li>
    </ul>
  </li>
</ul>

<h3 id="22-dbms-종류">2.2 DBMS 종류</h3>
<p>DBMS의 종류는 다양한데…(작성중)</p>

<p>참고사이트</p>
<ul>
  <li>https://learning-e.tistory.com/24?category=1011073</li>
  <li>https://coding-factory.tistory.com/entry/DB%EA%B8%B0%EC%B4%88-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%ED%8A%B9%EC%A7%95</li>
  <li>https://jess2.tistory.com/42</li>
</ul>]]></content><author><name>박마토</name></author><category term="Data" /><category term="Data" /><category term="DB" /><summary type="html"><![CDATA[안녕하세요. 요즘 DB와 SQL 공부를 하고 있는데요. 데이터 분석가로 일하다보니 DB와 SQL에 대해서 제대로 알아야 겠다는 생각이 드는 요즘입니다. 왜냐하면 대부분의 데이터들은 보통 데이터베이스에 보관되어 있고 우리는 그 데이터베이스에 접근하여 데이터를 가져와야 하기 때문이죠. 따라서 최근 토이프로젝트로 가계부 프로그램을 만들어보면서 DB를 직접 설계해보고 공부한 것들 그리고 SQL에 대해서도 한 번 정리해보고자 합니다.]]></summary></entry><entry><title type="html">[커리어] 데이터 분석가란?</title><link href="http://localhost:4000/data/second-post/" rel="alternate" type="text/html" title="[커리어] 데이터 분석가란?" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-07T06:39:00+09:00</updated><id>http://localhost:4000/data/second-post</id><content type="html" xml:base="http://localhost:4000/data/second-post/"><![CDATA[<p>안녕하세요.<br /><br />
블로그 첫 포스팅으로 무엇을 할까 많은 고민 끝에 데이터 분석가 직무에 대해서 제 개인적인 생각을 풀어보고자 합니다. 더 넓게는 데이터 전문가 영역에 대한 얘기를 하게 될 것 같네요. 2010년대 후반에 빅데이터 분야가 미디어를 통해 일반 대중들에게 소개되기 시작하면서 데이터 전문가에 대한 관심이 폭발하게 됩니다(물론, 그 전부터 관련 분야는 성장하고 있었습니다). 초기에는 데이터 관련 전문가에 대한 구분과 정의가 모호했었지만 이제는 어느정도 자리를 잡아가는 것 같습니다. 다양한 데이터 관련 커리어 중에 먼저 데이터 분석가에 대해서 데이터 분석 실무에서 직접 느꼈던 것들과 그리고 최근 채용공고를 보면서(?) 정리한 생각들을 공유하고자 합니다. 실무에 계신 분들은 공감할 수 있고, 데이터 전문가로의 커리어를 희망하시는 분들에게는 도움이 될 수 있는 글이 됐으면 좋겠습니다.<br /></p>

<h2 id="1-데이터-분석가란">1. 데이터 분석가란?</h2>
<p>데이터 분석가에 대해 얘기하기 위해서는 우선 데이터 전문가가 데이터를 활용해 무엇을 하는지 알아야 합니다. 깊게 들어가면 내용이 방대하니 간단하게만 얘기하면 다음 두가지 정도일 것 같습니다.</p>
<blockquote>
  <ol>
    <li>데이터 기반 의사결정 지원</li>
    <li>데이터 관련 프로덕트 개발<br /></li>
  </ol>
</blockquote>

<p>데이터 분석가는 위 두가지 영역 중에서 첫번째 영역, 즉 자신이 가진 능력과 기술을 활용해 조직 혹은 고객이 데이터 기반 의사결정을 할 수 있도록 지원할 수 있는 전문가입니다. 물론, 데이터 분석가 또한 데이터 관련 프로덕트 개발을 하며 데이터 과학자도 데이터 기반 의사결정 지원을 합니다. 다만, 지극히 개인적인 의견으로는 데이터 분석가는 주로 첫번째 영역에 특화된 전문가라 생각합니다.<br />
자 그렇다면 이제 데이터 분석가에 대해서 감이 좀 오시나요? <br />
정리하자면, 데이터 분석가란 <strong>직, 간접적으로 데이터 기반 의사결정을 위한 가치를 창출해내는 사람들</strong>이라 생각하면 될 것 같습니다.</p>
<h2 id="2-데이터-분석가는-어떤-일을-할까">2. 데이터 분석가는 어떤 일을 할까?</h2>
<p>데이터 기반의 의사결정 체계를 구축하기 위해서는 많은 것들이 필요합니다. 예를 들어, 나열해보자면 다음과 같은 것들이 있습니다.</p>
<ul>
  <li>문제 정의, 데이터 분석 기획, 결과 도출, 리포트 작성, 프리젠테이션, 커뮤니케이션</li>
  <li>데이터 수집, 가공, 분석, 시각화할 수 있는 시스템을 개발 혹은 운영</li>
  <li>데이터를 집계하여 의미있는 지표를 도출하고 모니터링할 수 있는 대시보드 개발</li>
  <li>머신러닝/딥러닝을 활용한 예측모형 개발 및 운영</li>
  <li>통계, 데이터 분석 관련 교육 및 컨설팅 등   <br /></li>
</ul>

<p>이 외에도, 산업, 조직 특성에 따라 다양한 일을 수행합니다. 적고나니 정말 많은 것들을 해야하네요. 큰 틀에서는 데이터 기반 의사결정 체계를 조직에 이식하기 위한 시스템을 설계하고 세부적으로는 분석을 통한 문제해결/인사이트 도출 등을 수행합니다. 본인 소속팀/직급/연차/스킬에 따라 수행하는 일이 모두 다르겠지만 데이터 분석가에게 가장 중요한 것은 결국 데이터로부터 의미있는 인사이트를 도출해내는 것입니다. 더 나아가 도출해낸 이 인사이트가 실제 비지니스 가치로 이어질 수 있도록 하기까지가 분석가의 역할입니다. 분석만 하고 끝내는 것은 학생 때나 하는 일입니다. 데이터 기반 의사결정을 하기 위해서는 정말 치열하게 고민해야 하고, 데이터에 기반한 객관적인 근거와 치밀한 논리가 필요합니다. 쉽지 않은 일이지만 그렇다고 못할 일도 아닙니다. 본인이 준비만 잘 되어 있다면 말이죠. <br />
(업무별 상세한 이야기는 내용이 길어지니 다음 포스트에서 다룰 수 있도록 하겠습니다.)<br /></p>
<h2 id="3-데이터-분석가는-어떤-능력을-필요로-할까">3. 데이터 분석가는 어떤 능력을 필요로 할까?</h2>
<p>데이터 분석가가 맡은 업무 범위가 넓다보니 정말 다양한 능력을 필요로 합니다. 데이터 분석가마다 본인의 필드에 맞는 특정한 스킬이 필요한 경우가 많지만, 공통적인 것들만 추려보겠습니다.</p>
<h3 id="31-통계수학-지식">3.1 통계/수학 지식</h3>
<p>통계와 수학 지식이 약하다면 데이터 분석 능력이 약할 수 밖에 없습니다. 당연히 통계와 수학을 잘하면 잘할수록 좋습니다. 필자는 경제 공부를 위해 석사 과정 내내 통계와 수학 공부만 하다보니 도움이 많이 됐습니다(계량 전공이었습니다). 수학을 배우면 증명을 통해 논리적, 비판적, 추상적 사고가 가능해지는데 이는 정말 중요합니다. 모든 업무가 그렇겠지만 데이터 분석은 처음부터 끝까지 논리싸움이기 때문입니다. 통계를 못하면 데이터 분석 자체가 불가합니다. 수학과 통계는 잘하면 잘할수록 좋습니다.</p>
<h3 id="32-데이터-분석">3.2 데이터 분석</h3>
<p>데이터 분석 능력을 수치화, 정량화 하기는 어렵습니다만 저는 개인적으로 데이터 분석 능력을 데이터를 통해 맥락을 잘 파악하고 이해할 수 있는 능력이라고 정의합니다. 이 부분에는 문제를 정의하고, 분석 프로세스를 기획하는 능력도 포함됩니다. 다루는 데이터의 크기가 방대하기 때문에 길을 못 잡으면 헤메는 경우가 많은데 경험이 많으신 분들은 금방 맥락을 짚고 중요 포인트를 찾아내는 능력을 갖추고 계신 분들이 많은 것 같습니다. 이 능력은 다른 분들이 분석한 내용을 찾아보고, 적용해보는 경험을 통해 증가하는 것 같습니다.</p>
<h3 id="33-프로그래밍">3.3 프로그래밍</h3>
<p>우리는 개발자나 프로그래머가 아니지만 프로그래밍 능력은 중요합니다. 왜냐하면 컴퓨터라는 도구를 활용하기 때문입니다. 프로그래밍 능력에 따라 업무 효율이 달라지게 됩니다. 코드를 잘 짜면 모델 학습이 1시간 돌거 30분 만에 완료되기도 하고, 데이터 전처리도 빠른 시간안에 끝낼 수 있기 때문입니다. 분석가가 마치 개발자처럼 0.몇초 성능 개선에 목숨거는 행태는 지양하지만 다루는 데이터의 크기가 커질수록 프로그래밍 능력의 중요도는 커질 것입니다. 두말하면 입 아픕니다. Python/R, SQL, 시각화 정도는 기본입니다.</p>
<h3 id="34-컴퓨터-공학">3.4 컴퓨터 공학</h3>
<p>스스로 고생을 많이한 분야인데, CS지식이 전무하다시피 한 상태에서 업무를 시작하니 너무 어려움이 많았습니다. 데이터베이스, 서버, API, NETWORK, 운영체제 등 넓은 CS지식을 필요로 합니다. IT 서비스를 제공하다 보니 관련 지식이 없이는 시스템 전반에 대해 이해하기 어렵습니다. 비전공자라면 개인적으로 시간을 할애해서 공부해야 합니다. CS 지식 없다면 관련 부서와 협업하기 힘들 확률이 높습니다.</p>
<h3 id="35-커뮤니케이션">3.5 커뮤니케이션</h3>
<p>대부분의 분석가들은 현업 담당자와 협업하는 형태의 프로젝트를 진행할 확률이 높습니다. 아무리 데이터 관련 분야가 성장하고 있다고 하더라도 제가 본 대부분 비 IT 현업 담당자분들(특히 제조업)은 데이터/IT에 대한 이해도가 굉장히 낮습니다. 뿐만 아니라, 본인들의 전문 분야에 외부 전문가가 들어오는 것을 처음에 굉장히 경계합니다. 이러한 장벽을 허물고 커뮤니케이션 할 수 있는 능력도 굉장히 중요합니다.</p>
<h3 id="36-도메인-지식">3.6 도메인 지식</h3>
<p>좋은 성과를 내기 위해서는 분석가가 도메인 지식에 대해서 잘 알아야 할 필요가 있습니다. 저도 1년은 지나니 현업 전문가와 편하게 얘기할 정도가 되더군요. 한 분야를 정해서 그 분야의 도메인 지식을 깊게 파는 것도 좋고, 어떤 도메인을 가던 그 분야에 지식을 빠르게 흡수하는 것도 좋을 것 같습니다. 결국 도메인 지식도 원활한 커뮤니케이션을 위해 필요로 합니다.</p>

<p>이 외에도 다양한 능력을 필요로 합니다. 시각화 화면을 예쁘게 디자인하는 능력도 가지고 있으면 분명한 플러스 요인이 될 수 있고, 문서를 잘 작성하는 능력도 중요합니다. 일일이 다 적자면 너무 길어지니 이 정도로 정리하겠습니다.</p>

<h2 id="4-맺음말">4. 맺음말</h2>
<p>요약하자면 데이터 분석가란 자신이 가진 능력을 활용해 데이터를 정리하고 데이터로부터 의미있는 가치를 창출해 성과를 내는 직업입니다. 데이터 분석가에 대해서 더 쓰고 싶은게 많은데 이는 다음 포스트에서 관련된 썰을 계속 풀어보겠습니다.<br />
저 스스로도 커리어 시작 전에는 데이터의 중요성에 대해서 긴가민가 했던게 사실이었습니다. 하지만 직접 프로젝트를 통해 데이터를 통한 문제해결을 경험하다보니 데이터의 가치를 인정하게 됐고 재미있고 보람차게 일하고 있습니다. 그럼 이번 포스트는 여기서 마치겠습니다. 부족한 글 읽어주셔서 감사합니다.</p>]]></content><author><name>박마토</name></author><category term="Data" /><category term="Data" /><category term="커리어" /><summary type="html"><![CDATA[안녕하세요. 블로그 첫 포스팅으로 무엇을 할까 많은 고민 끝에 데이터 분석가 직무에 대해서 제 개인적인 생각을 풀어보고자 합니다. 더 넓게는 데이터 전문가 영역에 대한 얘기를 하게 될 것 같네요. 2010년대 후반에 빅데이터 분야가 미디어를 통해 일반 대중들에게 소개되기 시작하면서 데이터 전문가에 대한 관심이 폭발하게 됩니다(물론, 그 전부터 관련 분야는 성장하고 있었습니다). 초기에는 데이터 관련 전문가에 대한 구분과 정의가 모호했었지만 이제는 어느정도 자리를 잡아가는 것 같습니다. 다양한 데이터 관련 커리어 중에 먼저 데이터 분석가에 대해서 데이터 분석 실무에서 직접 느꼈던 것들과 그리고 최근 채용공고를 보면서(?) 정리한 생각들을 공유하고자 합니다. 실무에 계신 분들은 공감할 수 있고, 데이터 전문가로의 커리어를 희망하시는 분들에게는 도움이 될 수 있는 글이 됐으면 좋겠습니다.]]></summary></entry><entry><title type="html">첫 포스팅 하기!</title><link href="http://localhost:4000/blog/first-post-test/" rel="alternate" type="text/html" title="첫 포스팅 하기!" /><published>2022-12-01T00:00:00+09:00</published><updated>2022-12-02T06:39:00+09:00</updated><id>http://localhost:4000/blog/first-post-test</id><content type="html" xml:base="http://localhost:4000/blog/first-post-test/"><![CDATA[<p>GitHub Blog를 활용한 첫 테스트 포스팅이다.
데이터 분석/개발/IT 쪽 관련 내용을 주로 포스팅할 예정이며,
가끔 일상 관련된 내용도 올리고자 한다.</p>

<p>내가 갖고 있는 지식과 스킬을 나누고
공유하여 모두 윈윈할 수 있는 그런 블로그가 되면 좋겠다.
소통의 장이랄까? 그런 블로그가 되길 원한다.</p>]]></content><author><name>박마토</name></author><category term="Blog" /><category term="Blog" /><summary type="html"><![CDATA[GitHub Blog를 활용한 첫 테스트 포스팅이다. 데이터 분석/개발/IT 쪽 관련 내용을 주로 포스팅할 예정이며, 가끔 일상 관련된 내용도 올리고자 한다.]]></summary></entry></feed>